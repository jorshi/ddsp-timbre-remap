{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from timbreremap.np import OnsetFrames\n",
    "from timbreremap.data import OnsetFeatureDataModule\n",
    "import timbreremap.feature as feature\n",
    "from timbreremap.synth import Snare808\n",
    "from timbreremap.tasks import TimbreRemappingTask\n",
    "from timbreremap.loss import FeatureDifferenceLoss\n",
    "from timbreremap.cli import _optimize_synth\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 48000\n",
    "data_path = \"audio/carson_gant_drums/performance.wav\"\n",
    "# data_path = 'audio/drumloop_1.wav'\n",
    "# data_path = 'audio/percussion_1.wav'\n",
    "\n",
    "N_ITERS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drums, sr = torchaudio.load(data_path)\n",
    "drums = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)(drums)[:1]\n",
    "print(drums.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(drums.squeeze().numpy(), rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_extractor = OnsetFrames(sr, 256)\n",
    "onset_times = onset_extractor.onset(drums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(drums[0].numpy())\n",
    "\n",
    "# Plot the detected onsets as vlines\n",
    "for onset in onset_times:\n",
    "    plt.vlines(onset, -0.5, 0.5, color=\"red\", alpha=0.5)\n",
    "\n",
    "plt.title(\"Detected Onsets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness_extractor = feature.Loudness(sample_rate=sample_rate)\n",
    "sc_extractor = feature.SpectralCentroid(sample_rate=sample_rate, scaling=\"kazazis\")\n",
    "sf_extractor = feature.SpectralFlatness()\n",
    "tc_extractor = feature.TemporalCentroid(sample_rate=sample_rate, scaling=\"schlauch\")\n",
    "\n",
    "frame_extractor = feature.CascadingFrameExtactor(\n",
    "    [loudness_extractor, sc_extractor, sf_extractor],\n",
    "    [\n",
    "        1,\n",
    "        4,\n",
    "    ],\n",
    "    2048,\n",
    "    512,\n",
    ")\n",
    "global_extractor = feature.CascadingFrameExtactor(\n",
    "    [tc_extractor],\n",
    "    [\n",
    "        1,\n",
    "    ],\n",
    "    5512,\n",
    "    5512,\n",
    ")\n",
    "extractor = feature.FeatureCollection([frame_extractor, global_extractor])\n",
    "\n",
    "onset_extractor = feature.CascadingFrameExtactor(\n",
    "    [loudness_extractor],\n",
    "    [\n",
    "        1,\n",
    "    ],\n",
    "    256,\n",
    "    256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_extractor = feature.MFCC(sample_rate=sample_rate, n_mfcc=13)\n",
    "frame_extractor = feature.CascadingFrameExtactor(\n",
    "    [loudness_extractor, mfcc_extractor], [1, 4], 2048, 512\n",
    ")\n",
    "extractor = feature.FeatureCollection([frame_extractor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = OnsetFeatureDataModule(\n",
    "    data_path, extractor, onset_extractor, sample_rate, return_norm=True\n",
    ")\n",
    "data.prepare_data(ref_idx=4)\n",
    "data.setup(\"fit\")\n",
    "dataset = data.train_dataset\n",
    "\n",
    "print(data.ref_idx)\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_map = list(data.loudsort.numpy())\n",
    "print(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for i, audio in enumerate(data.audio):\n",
    "    idx = idx_map.index(i)\n",
    "    feat = dataset[idx][1][None, ...]\n",
    "    feats.append(feat)\n",
    "    ipd.display(ipd.Audio(audio.squeeze().numpy(), rate=sample_rate))\n",
    "\n",
    "feats = torch.cat(feats, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"{k[0]}:{k[1]}\" for f in extractor.features for k in f.flattened_features]\n",
    "print(len(labels))\n",
    "\n",
    "# create subplots for each feature\n",
    "fig, axs = plt.subplots(feats.shape[-1], 1, figsize=(10, 3 * feats.shape[-1]))\n",
    "for i in range(feats.shape[-1]):\n",
    "    axs[i].plot(feats[:, i].numpy())\n",
    "    # axs[i].set_title(labels[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = Snare808(\n",
    "    sample_rate=sample_rate,\n",
    "    num_samples=sample_rate,\n",
    "    buffer_noise=True,\n",
    "    buffer_size=sample_rate,\n",
    ")\n",
    "\n",
    "preset = \"808_snare_1.json\"  # @param [\"808_snare_1.json\", \"808_snare_2.json\", \"808_snare_3.json\", \"808_noisy_snare.json\", \"808_open_snare.json\"]\n",
    "preset = f\"../cfg/presets/{preset}\"\n",
    "\n",
    "parameters, _ = synth.load_params_json(preset)\n",
    "audio = synth(parameters)\n",
    "ipd.Audio(audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a set of modulated presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Identity()\n",
    "task = TimbreRemappingTask(\n",
    "    model=model,\n",
    "    synth=synth,\n",
    "    feature=extractor,\n",
    "    loss_fn=FeatureDifferenceLoss(),\n",
    "    preset=preset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(task, target, ref_idx=None, iterations=100, target_scale=1.0):\n",
    "    data.prepare_data(ref_idx=ref_idx)\n",
    "    data.setup(\"fit\")\n",
    "    dataset = data.train_dataset\n",
    "\n",
    "    modulations = []\n",
    "    target_features = []\n",
    "    for d in dataset:\n",
    "        target = d[1][None, ...]\n",
    "        target = target * target_scale\n",
    "        target_features.append(target)\n",
    "        modulation = _optimize_synth(task, target, iterations=iterations, norm=d[2])\n",
    "        modulations.append(modulation)\n",
    "\n",
    "    return modulations, torch.cat(target_features, dim=0)\n",
    "\n",
    "\n",
    "modulations, target_features = run_optimization(task, feats, ref_idx=0, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resynthesize(mods, synth):\n",
    "    # Resynthesize the audio with the modulations\n",
    "    audio = []\n",
    "    synth = synth.to(\"cpu\")\n",
    "\n",
    "    # Get synth features\n",
    "    synth_audio = synth(parameters)\n",
    "    ref_features = extractor(synth_audio)\n",
    "    synth_features = []\n",
    "    idx_order = []\n",
    "\n",
    "    for i in range(len(mods)):\n",
    "        idx = idx_map.index(i)\n",
    "        params = parameters + mods[idx].detach().cpu()\n",
    "        params = torch.clip(params, 0.0, 1.0)\n",
    "        audio.append(synth(params))\n",
    "\n",
    "        pred_features = extractor(audio[-1])\n",
    "        diff = pred_features - ref_features\n",
    "        synth_features.append(diff)\n",
    "        idx_order.append(idx)\n",
    "\n",
    "    audio = torch.cat(audio, dim=0)\n",
    "\n",
    "    # Stitch back together with the onsets\n",
    "    resynth = torch.zeros_like(drums)\n",
    "    for i, onset in enumerate(onset_times):\n",
    "        start = onset\n",
    "        end = min(onset + audio[i].shape[-1], resynth.shape[-1])\n",
    "        resynth[0, start:end] += audio[i][: end - start]\n",
    "\n",
    "    return resynth, torch.cat(synth_features, dim=0), idx_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resynth, synth_features, idx_order = resynthesize(modulations, synth)\n",
    "\n",
    "ipd.Audio(resynth.detach().cpu().numpy(), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(target_features, synth_features, idx_order, extractor):\n",
    "    # create subplots for each feature\n",
    "    labels = [\n",
    "        f\"{k[0]}:{k[1]}\" for f in extractor.features for k in f.flattened_features\n",
    "    ]\n",
    "    fig, axs = plt.subplots(\n",
    "        target_features.shape[-1], 1, figsize=(10, 3 * feats.shape[-1])\n",
    "    )\n",
    "    for i in range(target_features.shape[-1]):\n",
    "        axs[i].plot(target_features[idx_order, i].numpy(), label=\"target\")\n",
    "        axs[i].plot(synth_features[:, i].numpy(), label=\"synth\")\n",
    "        # axs[i].set_title(labels[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_features(target_features, synth_features, idx_order, extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for i in range(len(dataset)):\n",
    "    print(f\"Optimizing for {i}\")\n",
    "    modulations, y_feat = run_optimization(task, feats, ref_idx=i, iterations=N_ITERS)\n",
    "    resynth, y_hat_faat, idx = resynthesize(modulations, synth)\n",
    "    audios.append(resynth.detach().cpu())\n",
    "\n",
    "    output = Path(f\"results/anchor_swap_{Path(data_path).stem}_{i}.png\")\n",
    "    fig = plot_features(y_feat, y_hat_faat, idx, extractor)\n",
    "    fig.savefig(output, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    torchaudio.save(output.with_suffix(\".wav\"), resynth.detach().cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = torch.hstack(audios[0:])\n",
    "\n",
    "output = Path(f\"results/anchor_swap_{Path(data_path).stem}_concat.wav\")\n",
    "torchaudio.save(output, audio_tensor, sample_rate)\n",
    "\n",
    "ipd.display(ipd.Audio(audio_tensor.squeeze().numpy(), rate=sample_rate))\n",
    "ipd.display(ipd.Audio(drums.squeeze().numpy(), rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = torch.hstack([torch.linspace(0.0, 0.75, 5), torch.linspace(1.0, 5.0, 5)])\n",
    "print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "\n",
    "for i in range(len(scale)):\n",
    "    print(f\"Optimizing for {i}\")\n",
    "    modulations, y_feat = run_optimization(\n",
    "        task, feats, ref_idx=0, iterations=N_ITERS, target_scale=scale[i]\n",
    "    )\n",
    "    resynth, y_hat_faat, idx = resynthesize(modulations, synth)\n",
    "    audios.append(resynth.detach().cpu())\n",
    "\n",
    "    output = Path(f\"results/target_scale_{Path(data_path).stem}_{i}.png\")\n",
    "    fig = plot_features(y_feat, y_hat_faat, idx, extractor)\n",
    "    fig.savefig(output, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    torchaudio.save(output.with_suffix(\".wav\"), resynth.detach().cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = torch.hstack(audios[0:])\n",
    "\n",
    "output = Path(f\"results/target_scale_{Path(data_path).stem}_concat.wav\")\n",
    "torchaudio.save(output, audio_tensor, sample_rate)\n",
    "\n",
    "ipd.display(ipd.Audio(audio_tensor.squeeze().numpy(), rate=sample_rate))\n",
    "ipd.display(ipd.Audio(drums.squeeze().numpy(), rate=sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
